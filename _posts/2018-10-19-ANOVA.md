---
layout: post
title: Analysis of Variance (ANOVA)
date: 2018-10-19 06:00
description: Explain how to test the significance between two or more means through analyzing their variances.
---

## Introduction

ANOVA is a statistical method used to test differences between two or more means by analyzing variance. It is similar to p-test, which is used to test differences between only two means. The non-specific null hypothesis, or omnibus null hypothesis, claims that all population means are equal. When the omnibus null hypothesis is rejected, the conclusion is that at least one population mean is different from at least one other mean. It is used to test general rather than specific differences, since ANOVA doesn't not reveal which means are different from which. The Tukey HSD test offers more specific differences, but it's not as commonly used as ANOVA.

### Assumptions made by ANOVA/t-test:

* Homogeneity of variance: the populations have the same variance.
* Normality: the populations are normally distributed.
* Independence: Each value is sampled independently from each other value. This assumption requires that each subject provide only one value. If a subject provides two scores, then the values are not independent. The analysis of data with two scores per subject is shown in the section on within-subjects ANOVA later in this chapter.

## Key concepts

### Factor and level

Factors refers to the number of independent variables that the experiment wants to measure. Level refers to the number of different settings within the factor. For example, one study can have three levels of age group within the age factor, or 5 different level of dosage within measuring the effect of a drug, or two levels within the gender factor(or three levels for non-binary as well!). You get the idea.

An ANOVA conducted on a design in which there is only one factor is called a **one-way ANOVA**. If an experiment has two factors, then the ANOVA is called a **two-way ANOVA**.

When all combinations of the levels are included, the design is called a **factorial design**. For example, a concise way of describing a double factor design is as a Gender (2) x Age (3) factorial design with the number of levels in parentheses.


### MSE and MSB

One estimate is called the mean square error (MSE) and is based on differences among scores within the groups. MSE estimates $$ \simga^2 $$ regardless of whether the null hypothesis is true (the population means are equal). The second estimate is called the mean square between (MSB) and is based on differences among the sample means. MSB only estimates $$ \simga^2 $$ if the population means are equal.

A standard method of determining whether the difference is the F ratio between MSB to MSE. By comparing the obtained F ratio in the F distribution, the probability density at that F ratio is the statistical value $$ p $$ that determines whether the null hypothesis can be rejected (e.g. $$ p < \alpha = 0.05 $$ ).


 $$
f(1,dfd) = t^2(df)
 $$

where dfd is the degrees of freedom for the denominator of the F test and df is the degrees of freedom for the t test. dfd will always equal df.

### Now you've learnt how to compute one-way (univariate) ANOVA. Can we extend it to a generalized form of multivariate ANOVA?

Let's see...
